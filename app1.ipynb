{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot v1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install chromadb\n",
    "# !pip install pypdf\n",
    "# !pip isntall pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromaPath = 'chroma/mxbai'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# def get_embedding_function():\n",
    "#     embeddings=BedrockEmbeddings(\n",
    "#         credentials_profile_name='default',region_name='us-east-1'\n",
    "#     )\n",
    "#     return embeddings\n",
    "\n",
    "def get_embedding_function():\n",
    "    embeddings=OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chroma-DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "# from get_embedding_function import get_embedding_function\n",
    "from langchain.vectorstores.chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "\n",
    "def load_documents():\n",
    "    DATA_PATH = 'Data'\n",
    "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    return document_loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "def split_document(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Docker Vulnerability Scan - Anchore\\nSecurity vulnerability scans on docker images can be done using Anchore. Anchore is an opensource tool to scan docker images. It has a \\nAnchore engine which scans the docker image and creates a scan report. Anchore also has a Anchore Jenkins plugin that can be used to \\nintegrate Anchore to the Jenkins. This Jenkins plugin can be used in the build pipelines.\\nThe plugin will send a list of docker images to the Anchore engine. The Anchore engine will pull the docker image and scans the image \\nbased on the configured policies. Once the scan is complete the result is send back to the Jenkins plugin. Based on the result the Jenkins \\nplugin will make the build stage failed/passed.' metadata={'source': 'Data/AVN-Docker Vulnerability Scan - Anchore-020524-074749.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "documents=load_documents()\n",
    "chunks=split_document(documents)\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chunk_ids(chunks):\n",
    "\n",
    "    # This will create IDs like \"data/monopoly.pdf:6:2\"\n",
    "    # Page Source : Page Number : Chunk Index\n",
    "\n",
    "    last_page_id = None\n",
    "    current_chunk_index = 0\n",
    "\n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get(\"source\")\n",
    "        page = chunk.metadata.get(\"page\")\n",
    "        current_page_id = f\"{source}:{page}\"\n",
    "\n",
    "        # If the page ID is the same as the last one, increment the index.\n",
    "        if current_page_id == last_page_id:\n",
    "            current_chunk_index += 1\n",
    "        else:\n",
    "            current_chunk_index = 0\n",
    "\n",
    "        # Calculate the chunk ID.\n",
    "        chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "        last_page_id = current_page_id\n",
    "\n",
    "        # Add it to the page meta-data.\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "def add_to_chroma(chunks: list[Document]):\n",
    "    # Load the existing database.\n",
    "    db = Chroma(\n",
    "        persist_directory=chromaPath, embedding_function=get_embedding_function()\n",
    "    )\n",
    "\n",
    "    # Calculate Page IDs.\n",
    "    chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n",
    "    # Add or Update the documents.\n",
    "    existing_items = db.get(include=[])  # IDs are always included by default\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
    "\n",
    "    # Only add documents that don't exist in the DB.\n",
    "    new_chunks = []\n",
    "    for chunk in chunks_with_ids:\n",
    "        if chunk.metadata[\"id\"] not in existing_ids:\n",
    "            new_chunks.append(chunk)\n",
    "\n",
    "    if len(new_chunks):\n",
    "        print(f\"ðŸ‘‰ Adding new documents: {len(new_chunks)}\")\n",
    "        new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
    "        db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "        db.persist()\n",
    "    else:\n",
    "        print(\"âœ… No new documents to add\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_database():\n",
    "    if os.path.exists(chromaPath):\n",
    "        shutil.rmtree(chromaPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(f):\n",
    "    if f=='reset':\n",
    "        print(\"âœ¨ Clearing Database\")\n",
    "        clear_database()\n",
    "    # Create (or update) the data store.\n",
    "    documents = load_documents()\n",
    "    chunks = split_document(documents)\n",
    "    add_to_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of existing documents in DB: 0\n",
      "ðŸ‘‰ Adding new documents: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kkalinga/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "main(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(query_text: str):\n",
    "    # Prepare the DB.\n",
    "    embedding_function = get_embedding_function()\n",
    "    db = Chroma(persist_directory=chromaPath, embedding_function=embedding_function)\n",
    "\n",
    "    # Search the DB.\n",
    "    results = db.similarity_search_with_score(query_text, k=5)\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    # print(prompt)\n",
    "\n",
    "    model = Ollama(model=\"llama3:latest\")\n",
    "    response_text = model.invoke(prompt)\n",
    "\n",
    "    sources = [doc.metadata.get(\"id\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: According to the given context, Grafana is an analytics and interactive data visualization tool that is multi-platform and open-source. It allows users to create dashboards with customizable graphs, charts, stats, and more, using data from various sources such as Prometheus, Graphite, ElasticSearch, MySQL, etc., shown as time-series analytics with timestamps.\n",
      "Sources: ['Data/AVN-Grafana-020524-074903.pdf:1:0', 'Data/AVN-Grafana-020524-074903.pdf:0:1', 'Data/AVN-Grafana-020524-074903.pdf:0:0', 'Data/AVN-Grafana-020524-074903.pdf:5:0', 'Data/AVN-Grafana-020524-074903.pdf:3:0']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'According to the given context, Grafana is an analytics and interactive data visualization tool that is multi-platform and open-source. It allows users to create dashboards with customizable graphs, charts, stats, and more, using data from various sources such as Prometheus, Graphite, ElasticSearch, MySQL, etc., shown as time-series analytics with timestamps.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_rag(\"What is Grafana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Based on the given context, Prometheus is a multi-dimensional data model with time series data identified by metric name and key/value pairs. It also comes with a flexible query language called PromQL to leverage this dimensionality.\n",
      "Sources: ['Data/AVN-Grafana-020524-074903.pdf:2:0', 'Data/AVN-Prometheus-020524-074948.pdf:0:1', 'Data/AVN-Grafana-020524-074903.pdf:6:0', 'Data/AVN-Prometheus-020524-074948.pdf:2:0', 'Data/AVN-Prometheus-020524-074948.pdf:3:0']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the given context, Prometheus is a multi-dimensional data model with time series data identified by metric name and key/value pairs. It also comes with a flexible query language called PromQL to leverage this dimensionality.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_rag(\"What is Prometheus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Based on the provided context, it can be inferred that:\n",
      "\n",
      "Grafana is a third-party tool used to display metrics collected by Prometheus. It is mentioned in Step 5 as an example Node Exporter dashboard for Grafana. This implies that Grafana is a visualization platform for time series data, allowing users to create dashboards and charts based on the metrics scraped from Prometheus targets.\n",
      "Sources: ['Data/AVN-Docker Vulnerability Scan - Anchore-020524-074749.pdf:0:1', 'Data/AVN-Prometheus-020524-074948.pdf:0:0', 'Data/AVN-Docker Vulnerability Scan - Anchore-020524-074749.pdf:0:0', 'Data/AVN-Grafana-020524-074903.pdf:0:1', 'Data/AVN-Grafana-020524-074903.pdf:4:0']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, it can be inferred that:\\n\\nGrafana is a third-party tool used to display metrics collected by Prometheus. It is mentioned in Step 5 as an example Node Exporter dashboard for Grafana. This implies that Grafana is a visualization platform for time series data, allowing users to create dashboards and charts based on the metrics scraped from Prometheus targets.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## mxbai\n",
    "query_rag(\"What is Grafana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
