{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot v1.1\n",
    "\n",
    "- Embedding model: OpenAI \n",
    "- LLM model: OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install chromadb\n",
    "# !pip install pypdf\n",
    "# !pip isntall pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "openai_api_key=os.getenv(\"openai_api_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromaPath= 'chroma/open'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceInferenceAPIEmbeddings\n",
    "\n",
    "def get_embedding_function():\n",
    "    embeddings=OpenAIEmbeddings(\n",
    "        # model=\"text-embedding-ada\",\n",
    "        api_key=openai_api_key\n",
    "    )\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.00318459689536844, 0.0110777294721545, -0.0041049622618212454]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings=get_embedding_function()\n",
    "text = \"This is a test document.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "query_result[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chroma-DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "# from get_embedding_function import get_embedding_function\n",
    "from langchain.vectorstores.chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "\n",
    "def load_documents():\n",
    "    DATA_PATH = 'Data'\n",
    "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    return document_loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "def split_document(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='How update a certificate into WAF ?\\nCurrently below endpoints are protected by WAF.\\nProduction:\\nhttps://aero-suite-prod-airarabia.accelaero.com/\\nhttps://aero-pay-prod-airarabia.accelaero.com\\nhttps://aero-pay-callbackapi-prod-airarabia.accelaero.com\\nStagingX\\nhttps://aero-suite-stage1-airarabia.isaaviation.net/ \\nWhat is the procedure to upload certificate into WAF ?\\n \\n1. Log into Oracle Console and navigate to Edge Policy Resources\\n   Oracle Console => Security and Identity => Web Application Firewall => OCI Edge Policy Resources\\n    Example: Oracle \\n2. Create a Certificate by providing SSL Certificate and Private Key.\\nWe need to do below additional step in our use cases as Oracle is unable to identify the encrypted version.' metadata={'source': 'Data/AVN-How update a certificate into WAF _-020524-111955.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "documents=load_documents()\n",
    "chunks=split_document(documents)\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chunk_ids(chunks):\n",
    "\n",
    "    # This will create IDs like \"data/monopoly.pdf:6:2\"\n",
    "    # Page Source : Page Number : Chunk Index\n",
    "\n",
    "    last_page_id = None\n",
    "    current_chunk_index = 0\n",
    "\n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get(\"source\")\n",
    "        page = chunk.metadata.get(\"page\")\n",
    "        current_page_id = f\"{source}:{page}\"\n",
    "\n",
    "        # If the page ID is the same as the last one, increment the index.\n",
    "        if current_page_id == last_page_id:\n",
    "            current_chunk_index += 1\n",
    "        else:\n",
    "            current_chunk_index = 0\n",
    "\n",
    "        # Calculate the chunk ID.\n",
    "        chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "        last_page_id = current_page_id\n",
    "\n",
    "        # Add it to the page meta-data.\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "def add_to_chroma(chunks: list[Document]):\n",
    "    # Load the existing database.\n",
    "    db = Chroma(\n",
    "        persist_directory=chromaPath, embedding_function=get_embedding_function()\n",
    "    )\n",
    "\n",
    "    # Calculate Page IDs.\n",
    "    chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n",
    "    # Add or Update the documents.\n",
    "    existing_items = db.get(include=[])  # IDs are always included by default\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
    "\n",
    "    # Only add documents that don't exist in the DB.\n",
    "    new_chunks = []\n",
    "    for chunk in chunks_with_ids:\n",
    "        if chunk.metadata[\"id\"] not in existing_ids:\n",
    "            new_chunks.append(chunk)\n",
    "\n",
    "    if len(new_chunks):\n",
    "        print(f\"👉 Adding new documents: {len(new_chunks)}\")\n",
    "        new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
    "        db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "        db.persist()\n",
    "    else:\n",
    "        print(\"✅ No new documents to add\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_database():\n",
    "    if os.path.exists(chromaPath):\n",
    "        shutil.rmtree(chromaPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(f):\n",
    "    if f=='reset':\n",
    "        print(\"✨ Clearing Database\")\n",
    "        clear_database()\n",
    "    # Create (or update) the data store.\n",
    "    documents = load_documents()\n",
    "    chunks = split_document(documents)\n",
    "    add_to_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of existing documents in DB: 65\n",
      "✅ No new documents to add\n"
     ]
    }
   ],
   "source": [
    "main(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "def query_rag(query_text: str):\n",
    "    # Prepare the DB.\n",
    "    embedding_function = get_embedding_function()\n",
    "    db = Chroma(persist_directory=chromaPath, embedding_function=embedding_function)\n",
    "\n",
    "    # Search the DB.\n",
    "    results = db.similarity_search_with_score(query_text, k=5)\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    print(prompt)\n",
    "\n",
    "    model = OpenAI(openai_api_key=openai_api_key,temperature=0.5)\n",
    "    response_text = model.invoke(prompt)\n",
    "\n",
    "    sources = [doc.metadata.get(\"id\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "Grafana\n",
      " \n",
      "Grafana is an analytics and interactive data visualization tool that is multi-platform and open-source. Mainly used when large amounts of \n",
      "data are needed to be monitored. The data is shown as time series analytics meaning shown using timestamps and these data can be \n",
      "taken from many data sources such as Prometheus, Graphite, ElasticSearch, MySQL etc. Grafana provides the capability of creating \n",
      "dashboards which can be customized by the user to include graphs, charts, stats and many more.\n",
      " \n",
      " \n",
      "Step 1:\n",
      " \n",
      "Install Grafana in a Docker container.\n",
      "An example of this is shown below.\n",
      "Have port 3000 mapped for Grafana since it is the default port number.\n",
      " \n",
      "Step 2:\n",
      " \n",
      "Open your browser and go to localhost:3000\n",
      "\n",
      "---\n",
      "\n",
      "Step 2:\n",
      " \n",
      "Open your browser and go to localhost:3000\n",
      "You will see the above page and type ‘admin’ for both username and password.What is Grafana?\n",
      "Getting Started\n",
      "1docker run -it -p 3000:3000 --net=host grafana/grafana\n",
      "\n",
      "---\n",
      "\n",
      "Prometheus\n",
      " \n",
      "            Prometheus, originally founded in 2012, is an open source system monitoring and alerting tool that has a active and large \n",
      "developer/user community. Prometheus is capable of collecting and storing metrics (numeric measurements) generated by the systems \n",
      "as time series data, meaning data are stored with the timestamp as well as alongside key-value pair optional label.  \n",
      " \n",
      " \n",
      " \n",
      "Prometheus Server will store the metrics that are scraped from Prometheus Targets and a Pushgateway will be needed as an \n",
      "intermediary for Short-lived Jobs.\n",
      "Alertmanager can be used for sending alerts to users.\n",
      "These metrics can be finally shown in the Prometheus web UI itself or third party tools such as Grafana.\n",
      "\n",
      "---\n",
      "\n",
      "Then you will see the Grafana homepage.\n",
      " \n",
      "Step 3:\n",
      " \n",
      "Navigate to Configuration > Data sources.\n",
      " \n",
      "Click “add data source” and choose “Prometheus”.\n",
      "\n",
      "---\n",
      "\n",
      "How to configure Grafana alerts for aeromart/revamp ?\n",
      "What is the objective to configure Grafana alerts ?\n",
      "Do we have the flexibility of creating alerts based on Elastic logs ?\n",
      "What are the available Strategies to configure alerts based on Elastic logs ?\n",
      "Dynamic trend based\n",
      "Static threshold based \n",
      "How to convert an existing Kibana filter as Grafana alert ?\n",
      "Illustration - 1\n",
      "Illustration - 2\n",
      "How we will get notified when there is a match for alert condition ?\n",
      "Screenshot\n",
      "Notification\n",
      "What is the objective to configure Grafana alerts ?\n",
      "This would enhance and improve our monitoring, to respond effectively to issues based on the alert triggered for our aeromart services.\n",
      "Do we have the flexibility of creating alerts based on Elastic logs ?\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: What is Grafana\n",
      "\n",
      "Response: \n",
      "Grafana is an analytics and interactive data visualization tool that is multi-platform and open-source. It is used for monitoring large amounts of data from various sources, such as Prometheus, Graphite, ElasticSearch, and MySQL, and displaying it as time series analytics. Grafana also allows users to create customized dashboards with graphs, charts, and statistics.\n",
      "Sources: ['Data/AVN-Grafana-020524-074903.pdf:0:0', 'Data/AVN-Grafana-020524-074903.pdf:0:1', 'Data/AVN-Prometheus-020524-074948.pdf:0:0', 'Data/AVN-Grafana-020524-074903.pdf:1:0', 'Data/revamp _-020524-112059.pdf:0:0']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGrafana is an analytics and interactive data visualization tool that is multi-platform and open-source. It is used for monitoring large amounts of data from various sources, such as Prometheus, Graphite, ElasticSearch, and MySQL, and displaying it as time series analytics. Grafana also allows users to create customized dashboards with graphs, charts, and statistics.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpt4\n",
    "query_rag(\"What is Grafana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "Prometheus\n",
      " \n",
      "            Prometheus, originally founded in 2012, is an open source system monitoring and alerting tool that has a active and large \n",
      "developer/user community. Prometheus is capable of collecting and storing metrics (numeric measurements) generated by the systems \n",
      "as time series data, meaning data are stored with the timestamp as well as alongside key-value pair optional label.  \n",
      " \n",
      " \n",
      " \n",
      "Prometheus Server will store the metrics that are scraped from Prometheus Targets and a Pushgateway will be needed as an \n",
      "intermediary for Short-lived Jobs.\n",
      "Alertmanager can be used for sending alerts to users.\n",
      "These metrics can be finally shown in the Prometheus web UI itself or third party tools such as Grafana.\n",
      "\n",
      "---\n",
      "\n",
      "Multi-dimensional data model with time series data identified by metric name and key/value pairs.\n",
      "PromQL, a flexible query language to leverage this dimensionality.What is Prometheus?\n",
      "Architecture\n",
      "Features\n",
      "\n",
      "---\n",
      "\n",
      "No reliance on distributed storage meaning single server nodes are autonomous.\n",
      "Time series collection happens via a pull model over HTTP.\n",
      "Pushing time series is supported via an intermediary gateway.\n",
      "Targets are discovered via service discovery or static configuration.\n",
      "Multiple modes of graphing and dashboarding support.\n",
      " \n",
      " \n",
      "Step 1:\n",
      " \n",
      "Create the configuration file for Prometheus. (Default location and file: /etc/prometheus/prometheus.yml)\n",
      " \n",
      "Below is a basic prometheus.yml file.\n",
      " \n",
      " \n",
      " \n",
      "The “job_name” will give a name for the metrics that are scraped.\n",
      "The “scrape_interval” will set how often the scraping will happen.\n",
      "The “targets” will set the port that will be scraped.\n",
      "Read https://prometheus.io/docs/prometheus/latest/configuration/configuration/ for further clarification.\n",
      " \n",
      " \n",
      " \n",
      "Step 2:\n",
      "\n",
      "---\n",
      "\n",
      "Give a Name and Folder with the Prometheus as the data source.\n",
      "Click “Import”.\n",
      " \n",
      "You should be able to see the imported Dashboard with the metrics.\n",
      "\n",
      "---\n",
      "\n",
      "Then you will see the Grafana homepage.\n",
      " \n",
      "Step 3:\n",
      " \n",
      "Navigate to Configuration > Data sources.\n",
      " \n",
      "Click “add data source” and choose “Prometheus”.\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: What is Prometheus\n",
      "\n",
      "Response: \n",
      "Prometheus is an open source system monitoring and alerting tool that is capable of collecting and storing metrics generated by systems as time series data. It has a multi-dimensional data model with a flexible query language and can be used for monitoring and alerting in single server nodes. It supports both pull and push models for collecting time series data and can be integrated with third party tools such as Grafana for visualization.\n",
      "Sources: ['Data/AVN-Prometheus-020524-074948.pdf:0:0', 'Data/AVN-Prometheus-020524-074948.pdf:0:1', 'Data/AVN-Prometheus-020524-074948.pdf:1:0', 'Data/AVN-Grafana-020524-074903.pdf:6:0', 'Data/AVN-Grafana-020524-074903.pdf:1:0']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nPrometheus is an open source system monitoring and alerting tool that is capable of collecting and storing metrics generated by systems as time series data. It has a multi-dimensional data model with a flexible query language and can be used for monitoring and alerting in single server nodes. It supports both pull and push models for collecting time series data and can be integrated with third party tools such as Grafana for visualization.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_rag(\"What is Prometheus\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
